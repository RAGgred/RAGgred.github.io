---
title: "CNN vs ANN: Image Classification using CIFAR-10"
date: 2025-04-15 14:11:00 +0100
categories: [machine learning]
tags: [cnn, ann]
render_with_liquid: false
---




In this project, I explore the effectiveness of **Convolutional Neural Networks (CNNs)** compared to traditional **Artificial Neural Networks (ANNs)** for image classification. Leveraging the **CIFAR-10** dataset, the task involves categorizing 32x32 color images across 10 distinct classes such as airplane, automobile, bird, cat, and more.


## üß† Key Differences

Convolutional Neural Networks have revolutionised the field of computer vision, particularly the task of image classification. CNNs emerged from the study of the brain‚Äôs visual cortex and are a type of deep neural network specifically designed to process images. The key difference between the two, and why CNNs are better at image classification lays in their architecture (Geron, 2022).

An ANN is a series of algorithms, composed of layers, usually an input layer, several hidden layers and an output layer of connected neurons (Awan, n.d). The way the information flows in an ANN is in a forward manner where the weighted sum of the signals arriving at the input is then passed through an activation function (usually a sigmoid or Tahn) which gives it its output (Kubat, 2021).

During training, the network is presented with data, makes a prediction based on its current knowledge (known as weights and biases), and then measures the accuracy by measuring the error rate which is calculated by dividing the number of errors by the number of examples that have been classified. Then backpropagation is used to find the best weights and biases so that the error rate decreases (Kubat, 2021; Geron 2022).

CNNs on the other hand use principles from linear algebra notably matrix multiplication, to identify patterns (Bhardwaj, 2022).

A CNN is usually composed of a few convolutional layers, each one generally followed by a ReLU layer, then a pooling layer, then another few convolutional layers followed by ReLU then pooling and so on with the image getting smaller and smaller as it goes through the network (Geron, 2022). This is followed by a regular ANN (Burkov 2019, Geron, 2022). With each layer, the CNN increases in its complexity, identifying greater portions of the image (IBM, 2021).
The convolutional layer is the core of a CNN, and requires input data, a filter, and a feature map. A filter (or kernel) is a small 2D array of weights, typically 3x3, that moves across the image‚Äôs receptive fields to detect features‚Äîa process called convolution. At each step, the filter computes a dot product between its weights and the corresponding input pixels, producing an output value. The filter then shifts by a stride and repeats this until it covers the entire image. The resulting collection of outputs forms the feature map (or activation map) (IBM, 2021; Geron, 2022, Kubat, 2021). CNNs are used for image classification because they can learn many layers of feature representations, consider the local context information and since there are fewer units, the number of parameters the model needs to learn are significantly lower (Bhardwaj, 2022; Kalra, 2023; Geron, 2022).

---

 ## Full code 
 
 You can check out the full code used for this project [here.](https://github.com/RAGgred/RAGgred.github.io/blob/main/assets/projects/notebooks/CNN.ipynb)

---

## üìä Dataset Summary

- **Source**: CIFAR-10 (Krizhevsky, 2009)  
- **Images**: 60,000 color images (32x32x3)  
- **Classes**: 10  
- **Train/Test Split**: 50,000/10,000  
- **Challenge**: Class imbalance across training batches

---

## üèóÔ∏è Model Architecture Comparison

### ANN Structure:
```python
model = Sequential([
    Dense(3000, activation='relu', input_shape=(3072,)),
    Dense(1000, activation='relu'),
    Dense(10, activation='softmax')
])
```

### CNN Structure (Final Model):
```python
model = Sequential([
    Input(shape=(32, 32, 3)),

    Conv2D(32, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),
    Dropout(0.25),

    Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),
    Dropout(0.25),

    Conv2D(128, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001)),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),
    Dropout(0.25),

    Flatten(),
    Dense(256, activation='relu', kernel_regularizer=l2(0.001)),
    Dropout(0.5),
    Dense(10, activation='softmax')
])
```

---

### üõ†Ô∏è CNN components:

1.	1 x input layer (32x32 colour images)
2.	3 x 2D Convolutional layers with same padding, and L2 regularisation and batch normalisation
3.	3 x Max Pooling layers following each Conv2D layer
4.	3 x 25 % Dropout layers following each Max Pooling layer
5.	1 x Flatten layer 
6.	1 x Dense layer with 256 neurons, ReLU activation, L2 kernel regularisation, with 50% dropout
7.	L1 dense layer with 10 neurons and Softmax activation 

![CNN Architecture](https://RAGgred.github.io/assets/projects/images/cnnarchitecture.png)


The Convolutional layers use 32, 64 and 128 filters, which are used to capture a range of features. The stride is set at the defaulted state (1,1) which means the filter is moving one pixel at the time with high degree of fit. This allows for a strong kernel presence. This was chosen because the images in the dataset are very small, and a larger stride could result in loss of important details. The model has same padding which helps spatial information. L2 regularisation was applied to prevent overfitting, and batch normalisation is used to normalise the activations withing each batch (Kubat, 2021; Geron, 2022). 

The model uses MaxPooling with a pool size of (2,2) used to reduce the size of the data. A small pool size was chosen due to the size of the images, as excessive reduction could mean loss of important detail (Kubat, 2021; Geron, 2022). 

Data augmentation has been used as a way to increase the size of the training set, as SMOTE (synthetic minority oversampling technique) to mitigate the imbalance in classes, and as a way to make the model more robust to noise. The images have been augmented by modifying the rotation, height and width, distortion, zoom effect and flip. This forces the model to be more tolerant to variations in position (Geron, 2022).

![ReLU](https://RAGgred.github.io/assets/projects/images/relu.png){: width="972" height="589" .w-50 .left}
ReLU or Rectified Linear Unit activation function was chosen for the hidden layers. It introduces non-linearity, enabling the model to learn complex patterns, as the output values are no longer limited to a fixed interval (Kubat, 2021). The fixed interval is a drawback of both Sigmoid and Tanh functions which are popular in ANNs. They saturate, large inputs snap to 1 (sigmoid) or -1 (Tanh), and small inputs snap to 0. They are most sensitive near their mid-points (0.5 for sigmoid, 0 for Tanh). Saturation limits sensitivity and makes it difficult for the learning algorithm to update the weights effectively (Brownlee, 2019).

The rectified linear activation function is a straightforward computation that outputs the input value as-is if it is greater than 0.0, or returns 0.0 if the input is 0.0 or less.

Whilst there are other functions such as GELU or Swish that vastly outperforms ReLU when it comes to the performance and the smoothness of the gradient descent, they are too complex for the current task, and the added computation is not justified in this case (Geron, 2022). 

ReLU however is computationally efficient and mitigates the vanishing gradient problem compared to sigmoid or tanh functions (Kubat, 2021), and promotes sparsity in the network, which can improve generalization (Krishnamurthy, 2022)

![softmax](https://RAGgred.github.io/assets/projects/images/softmax.png){: width="972" height="589" .w-50 .right}
For the output layer, SoftMax activation is used as it converts output values into probabilities by making the numbers in the vector sum to 1 (Kubat, 2021).
This equation calculates the probability of xi as a part of a set of inputs by normalizing it over the sum of exponentials of all inputs.





![cross entropy](https://RAGgred.github.io/assets/projects/images/crossentropy.png)
There are two types of cross entropy loss: binary (used in binary classification tasks with only two possible classes: true/false or positive and negative).
And categorical cross entropy loss, used in classification tasks with multiple classes. By minimizing loss, the model learns to assign higher probabilities to the correct class while reducing the probabilities for incorrect classes, improving accuracy (365 Data Science, 2021). 

The model uses categorical cross entropy loss, where the output is one-hot encoded. This loss function measures the distance between the predicted probability distribution and the true distribution (ground truth), encouraging the model to assign higher probabilities to the correct class (Kubat, 2021)

In another words the cross-entropy loss function measures the model‚Äôs performance by transforming its variables into real numbers, thereby evaluating the ‚Äôloss‚Äô associated with them. The higher the difference between the two, the higher the loss (365 Data Science, 2021; Kubat, 2021). 

---

## üîß Training Techniques & Strategy

- **Stratified Sampling** to maintain class balance
- **Data Augmentation** using:
  - Rotation
  - Width/Height shift
  - Zoom
  - Horizontal Flip
- **Loss Function**: Categorical Cross Entropy
- **Optimizer**: Adam
- **Regularization**: L2 Kernel Regularization
- **Early Stopping** after 5 epochs without improvement
- **Evaluation**: Stratified K-Fold Cross Validation (5 folds)

---

##  Performance Progression

| Model Version | Test Accuracy | Notes                        |
|---------------|---------------|-------------------------------|
| Model 14       | 54%           | No sampling or augmentation  |
| Model 19      | **85%**       | With full optimizations      |

##  Sample Predictions and model performance

| Actual Class | Predicted Class |
|--------------|-----------------|
| Ship | Truck |
| Cat | Bird |
| Frog | Frog  |

![Predicted vs actual](https://RAGgred.github.io/assets/projects/images/cnntest5-e.png)

The images show the predicted classes and the correct classes for 10 random examples, where the model correctly classified 8 out of 10 examples

###  Final Model performance

![Model performance](https://RAGgred.github.io/assets/projects/images/modelperformance.png){: width="972" height="589" .w-50 .left}
By the final fold the test accuracy increased to 84%, with an overall reasonable performance, especially the model‚Äôs balance between the precision and recall. The Model is slightly overfitted but not enough to cause any worry as seen from the validation accuracy, which at some points is lower than the training accuracy. 







###  Confusion Matrix (Final Model)

![Confusion Matrix](https://RAGgred.github.io/assets/projects/images/cnntest5-b.png){: width="972" height="589" .w-50 .right}
The confusion matrix shows that the model performs well across most categories, as shown by the strong diagonal line, but there are some confusions between truck and ship and bird and cat which suggests that further tuning can be made. The classification score shows consistent performance across all classes with balanced performance across all categories. The picture next to it is a random example for which the model predicted its class next to its actual class.






### Classification report

![Classification report](https://RAGgred.github.io/assets/projects/images/cnntest5-c.png)

---

## üß† Key Learnings

-  **CNNs vastly outperform ANNs** on image data due to spatial feature extraction.
-  **Stratified k-fold CV** helps mitigate class imbalance.
-  **Data augmentation** significantly improves generalization.
-  **Regular logging and incremental tuning** are crucial for tracking improvements.

---

## üöÄ Improvement sugestions

- Try **advanced activations** like Swish or GELU
- Benchmark against **ResNet/VGG** models
- Explore **automated hyperparameter tuning** (e.g., Optuna)

## üìö References

- 365 Data Science. (2021). What Is Cross-Entropy Loss? [online] Available at: https://365datascience.com/tutorials/machine-learning-tutorials/cross-entropy-loss/
- Acharya, A. (2023). Training, Validation, Test Split for Machine Learning Datasets. [online] Encord. Available at: https://encord.com/blog/train-val-test-split/
- Awan, A.A. (n.d.). What are Neural Networks? [online] DataCamp. Available at: https://www.datacamp.com/blog/what-are-neural-networks
- Baeldung. (2021). Stratified Sampling in Machine Learning. [online] Baeldung. Available at: https://www.baeldung.com/cs/ml-stratified-sampling [Accessed 18 Jan. 2025]
- Bhardwaj, D. (2022). Why CNN Performs Better than ANN on Image Classification. [online] Medium. Available at: https://medium.com/@divyanshub2311/why-cnn-performs-better-than-ann-on-image-classification-7f92e5a92904
- Brownlee, J. (2018, 2019). Machine Learning Mastery. [online] Available at: https://machinelearningmastery.com
- Buhl, N. (2023). Training, Validation, Test Split for Machine Learning Datasets. [online] -Encord. Available at: https://encord.com/blog/train-val-test-split/
- Burkov, A. (2019, 2020). The Hundred-Page Machine Learning Book & Machine Learning Engineering. Qu√©bec, Canada: True Positive Inc.
- G√©ron, A. (2022). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow. 3rd ed. O‚ÄôReilly Media, Inc.
- GeeksforGeeks. (2024). Stratified Sampling in Machine Learning. [online] Available at: https://www.geeksforgeeks.org/stratified-sampling-in-machine-learning/
- Gillis, A.S. (2024). What is a Validation Set? [online] WhatIs.com. Available at: https://www.techtarget.com/whatis/definition/validation-set
- Hall, M. (2019). Simple Random vs. Stratified Random Sample: What‚Äôs the Difference? [online] Investopedia. Available at: https://www.investopedia.com/ask/answers/042415/what-difference-between-simple-random-sample-and-stratified-random-sample.asp
- Huyen, C. (2022). Designing Machine Learning Systems: An Iterative Process for Production-Ready Applications. 1st ed. O'Reilly.
- IBM. (2021). Convolutional Neural Networks. [online] Available at: https://www.ibm.com/think/topics/convolutional-neural-networks
- Ibrahim, M. (2025). A Deep Dive Into Learning Curves in Machine Learning. [online] Weights & Biases. Available at: https://wandb.ai/mostafaibrahim17/ml-articles/reports/A-Deep-Dive-Into-Learning-Curves-in-Machine-Learning--Vmlldzo0NjA1ODY0 [Accessed 19 Jan. 2025]
- Kalra, K. (2023). Convolutional Neural Networks for Image Classification: Structure, Advantages, Limitations, and Current Advances. [online] Medium. Available at: https://medium.com/@khwabkalra1/convolutional-neural-networks-for-image-classification-f0754f7b94aa
- Krishnamurthy, B. (2022). ReLU Activation Function Explained. [online] BuiltIn. Available at: https://builtin.com/machine-learning/relu-activation-function
- Krizhevsky, A. (2009). Learning Multiple Layers of Features from Tiny Images. [online] Available at: https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf
- Kubat, M. (2021). An Introduction to Machine Learning. Cham: Springer International Publishing.
- Salahuddin, T., Haouari, F., Islam, F., Ali, R., Al-Rasbi, S., Aboueata, N., Rezk, E. and -- Jaoua, A. (2018). Breast Cancer Image Classification Using Pattern-Based Hyper Conceptual Sampling Method. Informatics in Medicine Unlocked, 13, pp.176‚Äì185. doi: https://doi.org/10.1016/j.imu.2018.07.002

---


